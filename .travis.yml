language: python

dist: bionic

cache: pip

python:
  - '3.7'

env:
  global:
  - DEBIAN_FRONTEND=noninteractive
  - POSTGRES_HOST=localhost
  - USASPENDING_DB_HOST=localhost
  - USASPENDING_DB_PORT=5432
  - USASPENDING_DB_USER=usaspending
  - USASPENDING_DB_PASSWORD=usaspender
  - USASPENDING_DB_NAME=data_store_api
  - DATABASE_URL=postgres://${USASPENDING_DB_USER}:${USASPENDING_DB_PASSWORD}@${USASPENDING_DB_HOST}:${USASPENDING_DB_PORT}/${USASPENDING_DB_NAME}
  - DOWNLOAD_DATABASE_URL=postgres://${USASPENDING_DB_USER}:${USASPENDING_DB_PASSWORD}@${USASPENDING_DB_HOST}:${USASPENDING_DB_PORT}/${USASPENDING_DB_NAME}
  - DJANGO_SETTINGS_MODULE='usaspending_api.settings'
  - ES_SCHEME=http
  - ES_HOST=localhost
  - ES_PORT=9200
  - ES_HOSTNAME=${ES_SCHEME}://${ES_HOST}:${ES_PORT}
  - BROKER_DB_HOST=localhost
  - BROKER_DB_PORT=5432
  - BROKER_DB_USER=admin
  - BROKER_DB_PASSWORD=root
  - BROKER_DB_NAME=data_broker
  - DATA_BROKER_DATABASE_URL=postgres://${BROKER_DB_USER}:${BROKER_DB_PASSWORD}@${BROKER_DB_HOST}:${BROKER_DB_PORT}/${BROKER_DB_NAME}
  - BROKER_REPO_URL=https://github.com/fedspendingtransparency/data-act-broker-backend.git
  - BROKER_REPO_BRANCH=$(if [ "${TRAVIS_EVENT_TYPE}" = "pull_request" ] && [ ! -z "`git ls-remote --heads ${BROKER_REPO_URL} ${TRAVIS_BRANCH}`" ]; then echo "${TRAVIS_BRANCH}"; else echo "qat"; fi;)
  - BROKER_REPO_FOLDER=${TRAVIS_BUILD_DIR}/../data-act-broker-backend
  - BROKER_DOCKER_IMAGE=dataact-broker-backend
  - GRANTS_API_KEY=${GRANTS_API_KEY}
  - MINIO_DATA_DIR=${HOME}/Development/data/usaspending/docker/usaspending-s3  # needs to be same place docker-compose will look for it (based on .env file)
  - MINIO_HOST=localhost
  - PYTEST_XDIST_NUMPROCESSES=4
  - COLUMNS=240  # for wider terminal output

jobs:
  # NOTE: See conftest.py pytest_collection_modifyitems for how Marks are assigned to tests
  include:
  - stage: Setup and Test
    name: Unit Tests
    env:
    - PYTEST_SETUP_TEST_DATABASES=false
    - PYTEST_PRELOAD_SPARK_JARS=false
    - PYTEST_INCLUDE_GLOB='test_*.py *_test.py'
    - PYTEST_EXCLUDE_GLOB=**/tests/integration/*
    - PYTEST_MATCH_EXPRESSION=
    - PYTEST_MARK_EXPRESSION='(not spark and not database and not elasticsearch)'
    workspaces:
      create:
        name: ws1
        paths:
          - job.*.txt
  - name: Non-Spark Integration Tests
    env:
    - PYTEST_SETUP_TEST_DATABASES=true
    - PYTEST_PRELOAD_SPARK_JARS=false
    - PYTEST_INCLUDE_GLOB=**/tests/integration/*
    - PYTEST_EXCLUDE_GLOB=
    - PYTEST_MATCH_EXPRESSION=
    - PYTEST_MARK_EXPRESSION='(not spark)'
    workspaces:
      create:
        name: ws2
        paths:
          - job.*.txt
  - name: Spark Integration Tests - test_load_to_from_delta.py
    env:
    - PYTEST_XDIST_NUMPROCESSES=4
    - PYTEST_SETUP_TEST_DATABASES=true
    - PYTEST_PRELOAD_SPARK_JARS=true
    - PYTEST_INCLUDE_GLOB='test_*.py *_test.py'
    - PYTEST_EXCLUDE_GLOB=
    - PYTEST_MATCH_EXPRESSION=test_load_to_from_delta.py
    - PYTEST_MARK_EXPRESSION=spark
    workspaces:
      create:
        name: ws3
        paths:
          - job.*.txt
  - name: Spark Integration Tests - test_load_transactions_in_delta.py
    env:
    # Concurrent sessions does not seem to be efficient for this test. Keeping at 1 session
    - PYTEST_XDIST_NUMPROCESSES=0
    - PYTEST_SETUP_TEST_DATABASES=true
    - PYTEST_PRELOAD_SPARK_JARS=true
    - PYTEST_INCLUDE_GLOB='test_*.py *_test.py'
    - PYTEST_EXCLUDE_GLOB=
    - PYTEST_MATCH_EXPRESSION=test_load_transactions_in_delta.py
    - PYTEST_MARK_EXPRESSION=spark
    workspaces:
      create:
        name: ws4
        paths:
          - job.*.txt
  - name: Spark Integration Tests - Other
    env:
    - PYTEST_XDIST_NUMPROCESSES=4
    - PYTEST_SETUP_TEST_DATABASES=true
    - PYTEST_PRELOAD_SPARK_JARS=true
    - PYTEST_INCLUDE_GLOB='test_*.py *_test.py'
    - PYTEST_EXCLUDE_GLOB=
    - PYTEST_MATCH_EXPRESSION='(not test_load_to_from_delta.py and not test_load_transactions_in_delta.py)'
    - PYTEST_MARK_EXPRESSION=spark
    workspaces:
      create:
        name: ws5
        paths:
          - job.*.txt
  - stage: Code Coverage
    workspaces:
      use: [ws1,ws2,ws3,ws4,ws5]
    script:
    - cat job.*.txt >> job.txt
    - cat job.txt

before_install:
  - sleep 1

install:
  - sleep 2

before_script:
  # Get dependencies to report code coverage to code climate
  - travis_retry curl -L https://codeclimate.com/downloads/test-reporter/test-reporter-latest-linux-amd64 > ./cc-test-reporter
  - chmod +x ./cc-test-reporter
  - ./cc-test-reporter before-build

script:
  - stty cols 240  # for wider terminal output
  - cd ${TRAVIS_BUILD_DIR}  # run build script out of repo dir
  - echo "${TRAVIS_JOB_NAME}" > job.$(echo $TRAVIS_JOB_NUMBER | cut -d'.' -f2).txt
  - cat job.*.txt
#after_script:
#  - ./cc-test-reporter after-build --exit-code $TRAVIS_TEST_RESULT
