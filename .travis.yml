language: python

dist: bionic

cache: pip

python:
  - '3.7'

env:
  global:
  - DEBIAN_FRONTEND=noninteractive
  - POSTGRES_HOST=localhost
  - USASPENDING_DB_HOST=localhost
  - USASPENDING_DB_PORT=5432
  - USASPENDING_DB_USER=usaspending
  - USASPENDING_DB_PASSWORD=usaspender
  - USASPENDING_DB_NAME=data_store_api
  - DATABASE_URL=postgres://${USASPENDING_DB_USER}:${USASPENDING_DB_PASSWORD}@${USASPENDING_DB_HOST}:${USASPENDING_DB_PORT}/${USASPENDING_DB_NAME}
  - DOWNLOAD_DATABASE_URL=postgres://${USASPENDING_DB_USER}:${USASPENDING_DB_PASSWORD}@${USASPENDING_DB_HOST}:${USASPENDING_DB_PORT}/${USASPENDING_DB_NAME}
  - DJANGO_SETTINGS_MODULE='usaspending_api.settings'
  - ES_SCHEME=http
  - ES_HOST=localhost
  - ES_PORT=9200
  - ES_HOSTNAME=${ES_SCHEME}://${ES_HOST}:${ES_PORT}
  - BROKER_DB_HOST=localhost
  - BROKER_DB_PORT=5432
  - BROKER_DB_USER=admin
  - BROKER_DB_PASSWORD=root
  - BROKER_DB_NAME=data_broker
  - DATA_BROKER_DATABASE_URL=postgres://${BROKER_DB_USER}:${BROKER_DB_PASSWORD}@${BROKER_DB_HOST}:${BROKER_DB_PORT}/${BROKER_DB_NAME}
  - BROKER_REPO_URL=https://github.com/fedspendingtransparency/data-act-broker-backend.git
  - BROKER_REPO_BRANCH=$(if [ "${TRAVIS_EVENT_TYPE}" = "pull_request" ] && [ ! -z "`git ls-remote --heads ${BROKER_REPO_URL} ${TRAVIS_BRANCH}`" ]; then echo "${TRAVIS_BRANCH}"; else echo "qat"; fi;)
  - BROKER_REPO_FOLDER=${TRAVIS_BUILD_DIR}/../data-act-broker-backend
  - BROKER_DOCKER_IMAGE=dataact-broker-backend
  - GRANTS_API_KEY=${GRANTS_API_KEY}
  - MINIO_DATA_DIR=${HOME}/Development/data/usaspending/docker/usaspending-s3  # needs to be same place docker-compose will look for it (based on .env file)
  - MINIO_HOST=localhost
  - PYTEST_XDIST_NUMPROCESSES=4
  - COLUMNS=240  # for wider terminal output
  - TRAVIS_JOB_INDEX="$(echo $TRAVIS_JOB_NUMBER | cut -d'.' -f2)"

jobs:
  include:
  - stage: Setup and Test
    # NOTE: See conftest.py pytest_collection_modifyitems for how Marks are assigned to tests
    name: Unit Tests
    env:
    - PYTEST_SETUP_TEST_DATABASES=false
    - PYTEST_PRELOAD_SPARK_JARS=false
    - PYTEST_INCLUDE_GLOB='test_*.py *_test.py'
    - PYTEST_EXCLUDE_GLOB=**/tests/integration/*
    - PYTEST_MATCH_EXPRESSION=
    - PYTEST_MARK_EXPRESSION='(not spark and not database and not elasticsearch)'
    workspaces:
      create:
        name: ws1
        paths:
          - job.*.txt
          - coverage.*.json
  - name: Non-Spark Integration Tests
    env:
    - PYTEST_SETUP_TEST_DATABASES=true
    - PYTEST_PRELOAD_SPARK_JARS=false
    - PYTEST_INCLUDE_GLOB=**/tests/integration/*
    - PYTEST_EXCLUDE_GLOB=
    - PYTEST_MATCH_EXPRESSION=
    - PYTEST_MARK_EXPRESSION='(not spark)'
    workspaces:
      create:
        name: ws2
        paths:
          - job.*.txt
          - coverage.*.json
  - name: Spark Integration Tests - test_load_to_from_delta.py
    env:
    - PYTEST_XDIST_NUMPROCESSES=4
    - PYTEST_SETUP_TEST_DATABASES=true
    - PYTEST_PRELOAD_SPARK_JARS=true
    - PYTEST_INCLUDE_GLOB='test_*.py *_test.py'
    - PYTEST_EXCLUDE_GLOB=
    - PYTEST_MATCH_EXPRESSION=test_load_to_from_delta.py
    - PYTEST_MARK_EXPRESSION=spark
    workspaces:
      create:
        name: ws3
        paths:
          - job.*.txt
          - coverage.*.json
  - name: Spark Integration Tests - test_load_transactions_in_delta.py
    env:
    # Concurrent sessions does not seem to be efficient for this test. Keeping at 1 session
    - PYTEST_XDIST_NUMPROCESSES=0
    - PYTEST_SETUP_TEST_DATABASES=true
    - PYTEST_PRELOAD_SPARK_JARS=true
    - PYTEST_INCLUDE_GLOB='test_*.py *_test.py'
    - PYTEST_EXCLUDE_GLOB=
    - PYTEST_MATCH_EXPRESSION=test_load_transactions_in_delta.py
    - PYTEST_MARK_EXPRESSION=spark
    workspaces:
      create:
        name: ws4
        paths:
          - job.*.txt
          - coverage.*.json
  - name: Spark Integration Tests - Other
    env:
    - PYTEST_XDIST_NUMPROCESSES=4
    - PYTEST_SETUP_TEST_DATABASES=true
    - PYTEST_PRELOAD_SPARK_JARS=true
    - PYTEST_INCLUDE_GLOB='test_*.py *_test.py'
    - PYTEST_EXCLUDE_GLOB=
    - PYTEST_MATCH_EXPRESSION='(not test_load_to_from_delta.py and not test_load_transactions_in_delta.py)'
    - PYTEST_MARK_EXPRESSION=spark
    workspaces:
      create:
        name: ws5
        paths:
          - job.*.txt
          - coverage.*.json
  - stage: Code Coverage
    env:
    - IS_CODE_COVERAGE_REPORT=true
    workspaces:
      use: [ws1,ws2,ws3,ws4,ws5]
    before_install: ""  # no-op
    install: ""  # no-op
    before_script:
      # Get dependencies to report code coverage to code climate
      - travis_retry curl -L https://codeclimate.com/downloads/test-reporter/test-reporter-latest-linux-amd64 > ./cc-test-reporter
      - chmod +x ./cc-test-reporter
    script:
    - cat job.*.txt >> jobs.txt
    - cat jobs.txt
    - for jf in job.*.txt; do if [[ "$(cat $jf | cut -d'|' -f3)" == 0 ]]; then ./cc-test-reporter format-coverage -t coverage.py -o ./coverage/codeclimate.$(echo "$jf" | cut -d'.' -f2).json coverage.$(echo "$jf" | cut -d'.' -f2).json; fi; done
    - ls ./coverage/
    - cat ./coverage/codeclimate.*.json
    - ./cc-test-reporter sum-coverage --output - --parts $(cat jobs.txt | wc -l) ./coverage/codeclimate.*.json | ./cc-test-reporter upload-coverage --input -; fi

before_install:
  - sleep 1

install:
  #- pip install pytest==7.4.* pytest-cov==4.1.*
  # setuptools is pinned to version 60.8.2 due to an error occurring with built-in version
  - travis_retry pip install setuptools==60.8.2
  - travis_retry pip install .[dev]
  - travis_retry pip install coveralls

before_script:
  # Log job details to be captured in workspace for this job, and used by code coverage stage
  - echo -n "${TRAVIS_JOB_INDEX}|${TRAVIS_JOB_NAME}" >> job.$TRAVIS_JOB_INDEX.txt
  - cat "job.$TRAVIS_JOB_INDEX.txt"
  # Get dependencies to report code coverage to code climate
  - travis_retry curl -L https://codeclimate.com/downloads/test-reporter/test-reporter-latest-linux-amd64 > ./cc-test-reporter
  - chmod +x ./cc-test-reporter
  - ./cc-test-reporter before-build

script:
  - stty cols 240  # for wider terminal output
  - cd ${TRAVIS_BUILD_DIR}  # run build script out of repo dir
  - pytest --cov=usaspending_api --cov-append --cov-report term --cov-report json:coverage.$TRAVIS_JOB_INDEX.json -k "$(pytest --collect-only --quiet --ignore-glob='**/tests/integration/*' -m '(not spark and not database and not elasticsearch)' --no-cov --disable-warnings | head -n $TRAVIS_JOB_INDEX | tail -n 1 | cut -d':' -f3)"
  - stat coverage.$TRAVIS_JOB_INDEX.json
  # Append build status to logged job details to be captured in workspace for this job, and used by code coverage stage
  - echo "|${TRAVIS_TEST_RESULT}" >> job.$TRAVIS_JOB_INDEX.txt
  - ls job.*.txt
  - cat "job.$TRAVIS_JOB_INDEX.txt"

after_script:
  # Append build status to logged job details to be captured in workspace for this job, and used by code coverage stage
#  - echo -n "|${TRAVIS_TEST_RESULT}" >> job.$(echo "$TRAVIS_JOB_NUMBER" | cut -d'.' -f2).txt
#  - ls job.*.txt
#  - cat job.*.txt
